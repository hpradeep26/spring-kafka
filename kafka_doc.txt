STEP 1: GET KAFKA
$ cd kafka_2.13-3.7.1
======================================
STEP 2: START THE KAFKA ENVIRONMENT
# Start the ZooKeeper service
bin\windows\zookeeper-server-start.bat config/zookeeper.properties

Open another terminal session and run:
# Start the Kafka broker service
bin\windows\kafka-server-start.bat config\server.properties

======================================
STEP 3: CREATE A TOPIC TO STORE YOUR EVENTS
bin\windows\kafka-topics.bat --create --topic quickstart-events --bootstrap-server localhost:9092
=========================================
A Kafka client communicates with the Kafka brokers via the network for writing (or reading) events. Once received, the brokers will store the events in a durable and fault-tolerant manner for as long as you need—even forever.

STEP 4: WRITE SOME EVENTS INTO THE TOPIC
create a producer 
bin\windows\kafka-console-producer.bat --topic quickstart-events --bootstrap-server localhost:9092

===========================================
STEP 5: READ THE EVENTS

create a consumer 
bin\windows\kafka-console-consumer.bat --topic quickstart-events --from-beginning --bootstrap-server localhost:9092

=======================================

What is event streaming?
-----------------------------
event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events.


Apache Kafka is the most popular open-source stream-processing software for collecting, processing, storing, and analyzing data at scale. 

. Most known for its excellent performance, low latency, fault tolerance, and high throughput, it's capable of handling thousands of messages per second. With over 1,000 Kafka use cases and counting, some common benefits are building data pipelines, leveraging real-time data streams, enabling operational metrics, and data integration across countless sources.

What can I use event streaming for?
=================================
--To process payments and financial transactions in real-time, such as in stock exchanges, banks, and insurances.
--To track and monitor cars, trucks, fleets, and shipments in real-time, such as in logistics and the automotive industry.
--To continuously capture and analyze sensor data from IoT devices or other equipment, such as in factories and wind parks.
--To collect and immediately react to customer interactions and orders, such as in retail, the hotel and travel industry, and mobile applications.
--To monitor patients in hospital care and predict changes in condition to ensure timely treatment in emergencies.
--

Apache Kafka® is an event streaming platform. What does that mean?
=================================
Kafka combines three key capabilities 
--To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems.
--To store streams of events durably and reliably for as long as you want.
--To process streams of events as they occur or retrospectively.
And all this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant, and secure manner.


Main Concepts and Terminology
==============================
An event records the fact that "something happened" in the world or in your business. It is also called record or message in the documentation. When you read or write data to Kafka, you do this in the form of events. Conceptually, an event has a key, value, timestamp, and optional metadata headers. Here's an example event:
Event key: "Alice"
Event value: "Made a payment of $200 to Bob"
Event timestamp: "Jun. 25, 2020 at 2:06 p.m."

Producers are those client applications that publish (write) events to Kafka, and consumers are those that subscribe to (read and process) these events. In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for.
 producers never need to wait for consumers.
 
 
Topics 
================== 
--Events are organized and durably stored in topics.a topic is similar to a folder in a filesystem, and the events are the files in that folder.

--An example topic name could be "payments". Topics in Kafka are always multi-producer and multi-subscriber:a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers that subscribe to these events.

--Events in a topic can be read as often as needed—unlike traditional messaging systems, events are not deleted after consumption. Instead, you define for how long Kafka should retain your events through a per-topic configuration setting, after which old events will be discarded. Kafka's performance is effectively constant with respect to data size, so storing data for a long time is perfectly fine.


Topics are partitioned, meaning a topic is spread over a number of "buckets" located on different Kafka brokers. 


Kafka APIs
-----------------
The Admin API to manage and inspect topics, brokers, and other Kafka objects.
The Producer API to publish (write) a stream of events to one or more Kafka topics.
The Consumer API to subscribe to (read) one or more topics and to process the stream of events produced to them.
The Kafka Streams API to implement stream processing applications and microservices. - Input is read from one or more topics in order to generate output to one or more topics, effectively transforming the input streams to output streams.
The Kafka Connect API to build and run reusable data import/export connectors that consume (read) or produce (write) streams of events from and to external systems and applications so they can integrate with Kafka.



1.2 Use Cases
============================
1.Messaging  -  Message brokers 
2.Website Activity Tracking -- Activity tracking  is often very high volume as many activity messages are generated for each user page view.
3.Metrics -- Kafka is often used for operational monitoring data
4.Log Aggregation - Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing
5.Stream Processing - Many users of Kafka process data in processing pipelines consisting of multiple stages, where raw input data is consumed from Kafka topics and then aggregated, enriched, or otherwise transformed into new topics for further consumption or follow-up processing
6.Event Sourcing - Event sourcing is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka's support for very large stored log data makes it an excellent backend for an application built in this style.
7. Commit Log


Location : /Users/sdudeja/Downloads/kafka-3.7.1-sr

Run the ZooKeeper : bin/zookeeper-server-start.sh config/zookeeper.properties
(If above command not running use : bin/zkServer.sh start; newer versions do not have zookeeper-server-start.sh script)

Run the Kafka Server :  bin/kafka-server-start.sh config/server.properties

Topic creation
bin/kafka-topics.sh --create --topic <topic-name> --bootstrap-server localhost:9092

bin/kafka-topics.sh --create --topic dummyTopic --bootstrap-server localhost:9092

Topic Creation with partitions
bin/kafka-topics.sh --create --topic driver-location-updates --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

Consumer
bin/kafka-console-consumer.sh --topic driver-location-updates --bootstrap-server localhost:9092 --from-beginning

Publish the event
echo '{"driverId": "12345", "latitude": 40.7128, "longitude": -74.0060, "timestamp": "2025-05-04T14:30:00Z"}' | bin/kafka-console-producer.sh --broker-list localhost:9092 --topic driver-location-updates


*********************

Topic creation with 2 partitions
bin/kafka-topics.sh --create --topic driver-location-updates-with-2-partitions --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1

Describe a topic
bin/kafka-topics.sh --describe --topic driver-location-updates-with-2-partitions --bootstrap-server localhost:9092

List Topics
bin/kafka-topics.sh --list --bootstrap-server localhost:9092

Create a single consumer without any group
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic driver-location-updates-with-2-partitions --from-beginning

Publish an Event on partition 1
echo "1:{\"driverId\": \"1\", \"latitude\": 28.7041, \"longitude\": 77.1025, \"timestamp\": \"2025-05-04T14:30:00Z\"}" | bin/kafka-console-producer.sh --broker-list localhost:9092 --topic driver-location-updates-with-2-partitions --property "parse.key=true" --property "key.separator=:"

echo "2:{\"driverId\": \"2\", \"latitude\": 28.6139, \"longitude\": 77.2090, \"timestamp\": \"2025-05-04T15:00:00Z\"}" | bin/kafka-console-producer.sh --broker-list localhost:9092 --topic driver-location-updates-with-2-partitions --property "parse.key=true" --property "key.separator=:"

Create a single consumer with group
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic driver-location-updates-with-2-partitions --group driver-location-consumer --from-beginning

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic driver-location-updates-with-2-partitions --group driver-location-consumer --from-beginning


Complete State of Consumers

bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group driver-location-consumer